{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSSEUBfq6Llf"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2tKJb586XlB"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpFTrKBA6fBI"
      },
      "source": [
        "text = \"The river was full of people sitting in the bank .\"\n",
        "\n",
        "# BERT requires the specific tokens before and after the text\n",
        "text_tokens = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Load pre-trained model and tokenize the text\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_text = tokenizer.tokenize(text_tokens)\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvROoBswHQ-b"
      },
      "source": [
        "Run the code above for the text below .. what do you see? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhFOgZFbHIOn"
      },
      "source": [
        "text = \"The river was full of people sitting in the banky .\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpIQ6PJpHbPl"
      },
      "source": [
        "Besides the tokenization we need to \n",
        "1. convert tokens to integers\n",
        "2. include the segment_ids (if one sentence should all be one)\n",
        "3. convert these to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVAuoRFPCTTM"
      },
      "source": [
        "indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_text])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs5nDtyUHzKW"
      },
      "source": [
        "Now we pass the text representation to the Bert model to take the Nx768 weights for our N input words. We will take them from Bert's last layer (in total 12)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iuP7ZbbDBhp"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(tokens_tensor, segments_tensors)\n",
        "last_layer= outputs.last_hidden_state  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBYyCmZ4IHXm"
      },
      "source": [
        "To be able to convert this tensor to a list to study it better we are performing a preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I4Dj6p8FYf1"
      },
      "source": [
        "token_embeddings = torch.squeeze(last_hidden_states, dim=0)\n",
        "list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "print('The list contains {}  words each of them has a vector of \"{}!\"'.format(len(list_token_embeddings),len(list_token_embeddings[0]))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UkuTNnJk9n"
      },
      "source": [
        "Ok before we proceed lets put eveything in a function. Input will be text and output will be the list_token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK86aYVJ7Dkm"
      },
      "source": [
        "def bert_embeddings(text):\n",
        "  # BERT requires the specific tokens before and after the text\n",
        "  text_tokens = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "  # Load pre-trained model and tokenize the text\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  tokenized_text = tokenizer.tokenize(text_tokens)\n",
        "  print(tokenized_text)\n",
        "  indexed_text = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_text])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "\n",
        "  # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "  model.eval()\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    last_layer= outputs.last_hidden_state  \n",
        "  \n",
        "  token_embeddings = torch.squeeze(last_layer, dim=0)\n",
        "  list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "  print('The list contains {}  words each of them has a vector of \"{}!\"'.format(len(list_token_embeddings),len(list_token_embeddings[0]))) \n",
        "\n",
        "  return list_token_embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e8wYT_mKiRG"
      },
      "source": [
        "#Now lets run it for multiple texts\n",
        "bank_river = bert_embeddings(\"the bank river\")\n",
        "bank_shore = bert_embeddings(\"the bank shore\")\n",
        "bank_thief = bert_embeddings(\"the bank thief\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3xGL0xZK_Hb"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "cos_dist = 1 - cosine(bank_river[2], bank_shore[2])\n",
        "print(cos_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0j0xvk7MRnw"
      },
      "source": [
        "cos_dist = 1 - cosine(bank_river[2], bank_thief[2])\n",
        "print(cos_dist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}