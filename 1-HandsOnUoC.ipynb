{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"1-HandsOnUoC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PNAdotgbGjv5"},"source":["# Hands on: Sentiment Analysis classification using different DNN architectures"]},{"cell_type":"markdown","metadata":{"id":"CmRbNyUsIkCB"},"source":["## Task\n","\n","The objective of sentiment analysis is to define the polarity of a sentence. For example in movie' s review the objective is to define the overall user satisfaction whether it is positive or negative. Here, we will not focus on aspect based sentiment analysis (polarity for specific aspects e.g actors, scenario, costume) but we will focus on a simple classification task of positive and negative polarity. \n","\n","We will train a model to predict for the polarity of a given sentence/paragraph. The input will be text and the output will be the positive or negative class. \n","\n","To perform this task we need data. Download and store your data under the data folder on the \"Colabnotebooks\" folder you have created for the project. \n","\n","The dataset is called aclImdb and contains movie reviews from a well known benchmark. The folder contains the training, test and validation data. \n","\n","We will \n","1. build the model using different architectures\n","2. train the model \n","3. evaluate the model' s accuracy \n","\n","The architectures we will use are\n","1. a simple linear regression model \n","2. an LSTM model \n","3. BERT model \n"]},{"cell_type":"markdown","metadata":{"id":"Tn2V3UuixZcU"},"source":["## Prepare notebook\n","Before you proceed make sure that your notebook "]},{"cell_type":"markdown","metadata":{"id":"9LyFi650Gjv_"},"source":["## Prepare data\n","You need to mount your google drive to be able to read and write on the drive. \n","During the reading operation the data will be read by the code and after training model will be stored under data/models folder. So make sure you create the folder in advance. To mount the drive just follow the instructions (go to url, choose the email account linked to this notebook and copy paste the url link in the window that appears below). "]},{"cell_type":"code","metadata":{"id":"AGA3TD4g3d1d"},"source":["https://github.com/mkoutsog/uoc_nlp_lectures.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lq8ikAFNMirl","executionInfo":{"status":"ok","timestamp":1621163737914,"user_tz":-120,"elapsed":1687,"user":{"displayName":"Maria Koutsogiannaki","photoUrl":"","userId":"07044758154797898175"}},"outputId":"319bd1c6-0904-491d-ab9a-b1c1e37279d4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZe1U5TY4r14","executionInfo":{"status":"ok","timestamp":1621163741162,"user_tz":-120,"elapsed":636,"user":{"displayName":"Maria Koutsogiannaki","photoUrl":"","userId":"07044758154797898175"}},"outputId":"8028f280-830b-429c-c5fa-c5cb1cbeed0d"},"source":["!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["drive  gdrive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itr0-WhN5Fvo","executionInfo":{"status":"ok","timestamp":1621163851088,"user_tz":-120,"elapsed":583,"user":{"displayName":"Maria Koutsogiannaki","photoUrl":"","userId":"07044758154797898175"}},"outputId":"e74a57da-c473-42cf-bef8-15a20dc37028"},"source":["%cd gdrive/MyDrive/UOC/invited_talk/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/UOC\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxKRyvAL5US3","executionInfo":{"status":"ok","timestamp":1621163894881,"user_tz":-120,"elapsed":637,"user":{"displayName":"Maria Koutsogiannaki","photoUrl":"","userId":"07044758154797898175"}},"outputId":"3c54ebd3-0470-4bc7-c98b-d3a7d63d1088"},"source":["!ls\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" 1703.01619.pdf\t\t  csd\n"," 1-HandsOnUoC.ipynb\t 'Project on Auditory Sig. Proc..ipynb'\n"," 2020summer_dl4nlp_labs   UOC-2021.gslides\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6K52wKjh_cIW"},"source":["# How to setup a Multi-Layer Perceptron model for imdb sentiment analysis in Keras\n","\n","def Snippet_380(): \n","\n","    print()\n","    print(format('How to setup a Multi-Layer Perceptron model for sentiment analysis in Keras','*^92'))\n","\n","    import time\n","    start_time = time.time()\n","\n","    # load libraries\n","    from keras.datasets import imdb\n","    from keras.models import Sequential\n","    from keras.layers import Dense, Flatten\n","    from keras.layers.embeddings import Embedding\n","    from keras.preprocessing import sequence\n","    \n","    # load data and Set the number of words we want\n","    top_words = 5000\n","    input_length = 500\n","\n","    # Load data and target vector from movie review data\n","    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","    \n","    print(); print(X_train.shape); print(X_train)\n","    print(); print(y_train.shape); print(y_train)    \n","    print(); print(X_test.shape);  print(X_test)\n","    print(); print(y_test.shape);  print(y_test)    \n","    \n","    # Convert movie review data to feature matrix\n","    X_train = sequence.pad_sequences(X_train, maxlen=input_length)\n","    print(); print(X_train.shape); print(X_train)\n","\n","    X_test = sequence.pad_sequences(X_test, maxlen=input_length)\n","    print(); print(X_test.shape);  print(X_test)\n","\n","    # setup a MLP network\n","    model = Sequential()\n","    model.add(Embedding(top_words, 32, input_length=input_length))\n","    model.add(Flatten())\n","    model.add(Dense(250, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","\n","    # Fit the model\n","    model.fit(X_train, y_train, validation_data=(X_test, y_test), \n","              epochs=20, batch_size=128, verbose=1)\n","\n","    # Final evaluation of the model\n","    scores = model.evaluate(X_test, y_test, verbose=1)\n","    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","\n","    print(); print(\"Execution Time %s seconds: \" % (time.time() - start_time))\n","\n","Snippet_380()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxLtV6AN5E3-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skPi81GPxZcW"},"source":["Bef\n","Install the packages and load libraries. \n"]},{"cell_type":"code","metadata":{"id":"-zCzpZ_VHou0"},"source":["# set seed for replicability of results\n","import numpy as np\n","import tensorflow as tf\n","\n","np.random.seed(1)\n","tf.random.set_seed(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKF90-hKGjwA"},"source":["# Load the data\n","import re\n","import pandas as pd\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    data = pd.DataFrame(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/2020summer_dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lgI0VTRTGjwH"},"source":["## 2. Examining the data"]},{"cell_type":"code","metadata":{"id":"NFk0e4C_GjwI"},"source":["# Print a sample of negative text chunks\n","training_set[training_set.label == 0].head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajuQm4N_GjwK"},"source":["# Print a sample of positive text chunks\n","training_set[training_set.label == 1].head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynwb61YYGjwN"},"source":["## 3. Preprocessing the data\n","Once data is loaded the next step is to preprocess it to obtain the vectorized form (i.e. the process of transforming text into numeric tensors), which basically consist of:\n","\n","- Tokenization, tipically segment the text into words. (Alternatively, we could segment text into characters, or extract n-grams of words or characters.)\n","- Definition of the dictionary index and vocabulary size (in this case we set to 1000 most frequent words)\n","- Transform each word into a vector. \n","\n","\n","There are multiple ways to vectorize tokens. The main two are the following: ___One-hot encoding___ and ___word embedding___. In this lab, we'll Keras basic tools to obtain the one-hot encoding, and we'll leave word embeddings for the successive labs. "]},{"cell_type":"code","metadata":{"id":"BvFwQCKuGjwP"},"source":["from sklearn.utils import shuffle\n","\n","# Shuffle dataset\n","training_set = shuffle(training_set)\n","dev_set = shuffle(dev_set)\n","test_set = shuffle(test_set)\n","\n","# Obtain text and label vectors, and tokenize the text\n","train_texts = training_set.text\n","train_labels = training_set.label\n","\n","dev_texts = dev_set.text\n","dev_labels = dev_set.label\n","\n","test_texts = test_set.text\n","test_labels = test_set.label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGoUeBbFvkYg"},"source":["print(training_set.loc[0])\n","print(train_labels.loc[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqenEnlgGjwR"},"source":["### 3.1. One-hot encoding of the data\n","\n","One-hot encoding is the most basic way to convert a token into a vectort. Here, we'll turn the input vectors into (0,1)-vectors. The process consist of associating a unique integer-index with every word in the vocabulary.\n","\n",">>>>>![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/vectorize_small.png)\n","\n","\n","For example, if the tokenized vector contains a word that its dictionary index is 14, then in the processed vector, the 14th entry of the vector will be 1 and the rest will set to 0.\n","\n","Note that when using keras built-in tools for indexing, ```0``` is a reserved index that won't be assigned to any word."]},{"cell_type":"code","metadata":{"id":"TC9YoC7mGjwR"},"source":["from tensorflow.keras.preprocessing import text, sequence\n","from tensorflow.keras import utils\n","\n","# Create a tokenize that takes the 1000 most common words\n","tokenizer = text.Tokenizer(num_words=1000)\n","\n","# Build the word index (dictionary)\n","tokenizer.fit_on_texts(train_texts) # Create word index using only training part\n","\n","# Vectorize texts into one-hot encoding representations\n","x_train = tokenizer.texts_to_matrix(train_texts, mode='binary')\n","x_dev = tokenizer.texts_to_matrix(dev_texts, mode='binary')\n","x_test = tokenizer.texts_to_matrix(test_texts, mode='binary')\n","          \n","# Converts the labels to a one-hot representation\n","y_train = train_labels\n","y_dev = dev_labels\n","y_test = test_labels\n","\n","print('Text of the first examples: \\n{}\\n'.format(train_texts.iloc[0]))\n","print('Vector of the first example:\\n{}\\n'.format(x_train[0]))\n","print('Binary representation of the output:\\n{}\\n'.format(y_train[0]))\n","\n","print('Shape of the training set (nb_examples, vector_size): {}\\n'.format(x_train.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qh5XtExSHovP"},"source":["# Recorver the word index that was created with the tokenizer\n","word_index = tokenizer.word_index\n","print('Found {} unique tokens.\\n'.format(len(word_index)))\n","word_count = tokenizer.word_counts\n","print(\"Show the most frequent word index:\")\n","for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=True)):\n","    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n","    if i == 9: \n","        print('')\n","        break\n","\n","for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=False)):\n","    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n","    if i == 9: \n","        print('')\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OQNmBMyWHovS"},"source":["Check what we obtain when we vectorize words that are out of the index (_out of vocabulary_ words)."]},{"cell_type":"code","metadata":{"id":"am6zdzHuHovU"},"source":["oov_sample = ['saddam', 'plausible']\n","sequences = tokenizer.texts_to_matrix(oov_sample)\n","print(sequences[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Zoa__wYHova"},"source":["It is possible to obtain the lists of integers indices instead of the one-hot binary representation."]},{"cell_type":"code","metadata":{"id":"316HZROEHova"},"source":["# Turns strings into list of integer indices\n","print(train_texts.iloc[0])\n","one_hot_results = tokenizer.texts_to_sequences(train_texts)\n","print(one_hot_results[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwTuDiROGjwU"},"source":["## 4. Building the model architecture\n","When we build a neural network we usually take into account the following points:\n","- The __layers__, and how they are combined (that is, the structure and parameters of the model)\n","- The __input__ and the __labeled output__ data that the model needs to map.\n","- __Loss function__ that signals how well the model is doing.\n","- The __optimizier__ which defines the learning procedure.\n","\n","In this very first session we'll keep all this very simple. Keras provide a simple framework for combining layers. There are available two types of classes for building the model: The _Sequential_ Class and the _functional_ API. The later is dedicated to DAGs structures, which let you to build arbitrary models. The former is for linear stacks of layers, which is the most common and simplest archicture. \n","\n","In this session, we will build a __logistic regression__ which is the most simple neural network model. More complicated models, like the multilayer perceptron, will be learnt in the next lab sessions.\n","\n","The logistic regression can be implemented in Keras using ```Dense``` layer. ```Dense``` implements the following  operation:\n","\n","> ```output = activation(dot(input, kernel) + bias)```\n","\n","where activation is the element-wise activation function passed as the ```activation``` argument (_sigmoid_ function in our case), ```kernel``` is a weights matrix created by the layer, and ```bias``` is a bias vector created by the layer.\n","\n","Remenber from the slides that mathematically can be written as follows:\n","> $sigmoid(W^{T}X + b)$\n","\n","\n","Regarding input data, we will use the __one-hot encoding__ (see previous sections). We'll set ```(binary) cross-entropy``` as a __loss function__ and ```adam```, a variant of the _Stochastic Gradient Descent_, as the __optimizer__.\n","\n","Feel free to explore different loss-functions (e.g. MSE) and optimizers (e.g. rmsprop) you can improve the model (see Exercise 2, below)."]},{"cell_type":"code","metadata":{"id":"b-B518nrGjwV"},"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n","print(input_size)\n","num_classes = 2\n","\n","model = Sequential()\n","model.add(Dense(units=1, activation='sigmoid', input_shape=(input_size,)))\n","# now the model will take as input arrays of shape (*, input_size)\n","# and output arrays of shape (*, 1)\n","\n","# Compile the model using a loss function and an optimizer.\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m-hu7SjOHovi"},"source":["### Exercise 1\n","Answer the following questions:\n","- How many layers has the model?\n","- What is the input size? \n","- How many parameters has the models? What is the relationship with the input size?"]},{"cell_type":"markdown","metadata":{"id":"JcqNFVwqGjwY"},"source":["## 5. Training the model\n","Next piece of code trains the model defined above. As you can see the way we train the model is very similar to scikit-learn.\n","\n","```model.fit()``` returns the history of the training, which contains the accuracies and loss values of training and development sets obtained in each of the 50 epoch."]},{"cell_type":"code","metadata":{"id":"sukZkeL5Gjwa"},"source":["# TODO Run the model. Feel free to experiment with different batch sizes and number of epochs.\n","history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8gDTG50-CCZ"},"source":["import matplotlib.pyplot as plt\n","\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9fM-CxjBU5N"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAXpHeYFGjwc"},"source":["## 6. Evaluating the model\n","\n","Once we fit the model we can use the method ```model.evaluate()``` to obtain the accuracy on test set."]},{"cell_type":"code","metadata":{"id":"vbhiXF5FGjwd"},"source":["score = model.evaluate(x_test, y_test, verbose=1)\n","print(\"Accuracy: \", score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oB-y83hQYmkt"},"source":["## 7. Model Tuning"]},{"cell_type":"markdown","metadata":{"id":"UgaTYpVaYrth"},"source":["### 7.1. Effect of Learning Rate "]},{"cell_type":"markdown","metadata":{"id":"qDtOC0MhHovy"},"source":["#### Exercise 2\n","The model in Section 4 uses default values of learning rate, and does not use any type of regularization. It would be great if you could try improving the model by exploring different parameters. You can explore the following hyperparameters: \n","- Optimizers: SGD, ADAGRAD, etc.\n","- Learning Rates of the optimizer.\n","- Regularization.\n","\n","You can check Keras API to learn how to use and set up different optimizers: \n","- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers (https://keras.io/optimizers/)\n","- https://www.tensorflow.org/api_docs/python/tf/keras/regularizers (https://keras.io/regularizers/)\n","\n","\n","In this session we'll focus in the importance of the learning rate. We'll compare a large and a small learning rate with the default one. \n","\n","\n","__Please run the following cells and answers to the next question:__\n","\n","- Why we obtain such a different plots with each learning rate?\n","\n","- What is the difference when comparing the following curves:\n","   - ```train large``` vs  ```train orig```\n","   - ```dev large``` vs  ```dev orig```\n","\n","- And the following ones:\n","   - ```train small``` vs  ```train orig```\n","   - ```dev small``` vs ```dev orig```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"brZ83u0YHovz"},"source":["# Example of using optimizer object\n","\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.optimizers import Adam\n","\n","model2 = Sequential()\n","\n","# add L2 weight regularization to logistic regression\n","regularizer = regularizers.l2(0.)\n","model2.add(Dense(units=1, activation='sigmoid', input_shape=(input_size,), kernel_regularizer=regularizer))\n","\n","# Init Optimizer\n","lr_small = Adam(learning_rate=0.00001)\n","lr_large = Adam(learning_rate=0.5)\n","\n","model2.compile(loss='binary_crossentropy', optimizer=lr_small, metrics=['accuracy'])\n","history_small_lr= model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n","\n","model2.compile(loss='binary_crossentropy', optimizer=lr_large, metrics=['accuracy'])\n","history_large_lr= model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WndYhv6vHov1"},"source":["import matplotlib.pyplot as plt\n","\n","# summarize history for accuracy\n","plt.plot(history_large_lr.history['accuracy'])\n","plt.plot(history_large_lr.history['val_accuracy'])\n","\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.plot(history_small_lr.history['accuracy'])\n","plt.plot(history_small_lr.history['val_accuracy'])\n","\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train large', 'dev large', 'train orig', 'dev orig', 'train small', 'dev small'], loc='center right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjqsvrmUYZve"},"source":["### 7.2. Effect of regularization"]},{"cell_type":"markdown","metadata":{"id":"YI7zRZ_dHov6"},"source":["#### Exercise 3\n","\n","In this session we'll focus in the effect of regularization. We'll compare regularized model agains non-regularized one.\n","\n","__Please run the following cell and answers to the next question__:\n","\n","What is the effect of including a regularization term? Is it always a good thing to be included?\n","\n","\n","------\n","\n","The plots might not be the expected, but you should note that we reduced the vocabulary to only 1000 most frequent   words in training. Anyway, you should see the differences of learning curves when training with and without regularization.\n","\n","----\n"]},{"cell_type":"code","metadata":{"id":"rkbN-GpQHov7"},"source":["import matplotlib.pyplot as plt\n","from tensorflow.keras import optimizers, regularizers\n","\n","model2 = Sequential()\n","\n","# add L2 weight regularization to logistic regression\n","regularizer = regularizers.l2(0.000001)\n","model2.add(Dense(units=1, activation='sigmoid', input_shape=(input_size,), kernel_regularizer=regularizer))\n","\n","# Init optimizer\n","opt = optimizers.Adam() \n","model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","history_reg = model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n","\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.plot(history_reg.history['accuracy'])\n","plt.plot(history_reg.history['val_accuracy'])\n","\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train orig', 'dev orig', 'train reg', 'dev reg'], loc='lower right')\n","plt.show()"],"execution_count":null,"outputs":[]}]}